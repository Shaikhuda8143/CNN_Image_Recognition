{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8a03861",
   "metadata": {},
   "source": [
    "## WHY?\n",
    "* Convolutional Neural Network or CNN is a type of artificial neural network, which is widely **used for image/object recognition\n",
    "and classification.** Deep Learning thus recognizes objects in an image by using a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908172b6",
   "metadata": {},
   "source": [
    "# 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1b32808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bb6e83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f6810ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten,Conv2D,Dense,Dropout,MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9870f429",
   "metadata": {},
   "source": [
    "### Uses\n",
    "* Sequential groups a linear stack of layers into a tf.keras.Model.\n",
    "* 'Conv2D' This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs\n",
    "* Downsamples the input along its spatial dimensions (height and width) by taking the maximum value over an input window (of size defined by pool_size) for each channel of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad63cce9",
   "metadata": {},
   "source": [
    "# 2. Import Data using Augmentation Technique\n",
    "* Data augmentation in data analysis are techniques used to **increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data.**\n",
    "\n",
    "* It acts as a regularizer and **helps reduce overfitting when training a machine learning model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17237e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3c46b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data   =ImageDataGenerator( shear_range=0.02,zoom_range=0.10,horizontal_flip=True,vertical_flip= True,rescale=1./255)\n",
    "validation_data =ImageDataGenerator(rescale=1./255)\n",
    "test_data       = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d8ed4d",
   "metadata": {},
   "source": [
    "### Rescale\n",
    "* Rescale is a value by which we will multiply the data before any other processing. Our original images consist in RGB coefficients in the 0-255, but such values would be too high for our model to process (given a typical learning rate), so we target values between 0 and 1 instead by scaling with a 1/255. factor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc12dcb",
   "metadata": {},
   "source": [
    "#### Tuple of integers (height, width), default: (256, 256) == target_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c844677e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28 images belonging to 3 classes.\n",
      "Found 6 images belonging to 3 classes.\n",
      "Found 6 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "training_data_path   =training_data.flow_from_directory(directory=r'C:\\CNN_multiclass_classifier\\Train',\n",
    "                                     target_size=(128, 128))\n",
    "validation_data_path =validation_data.flow_from_directory(directory=r'C:\\CNN_multiclass_classifier\\Validation',\n",
    "                                     target_size=(128, 128))\n",
    "test_data_path       =test_data.flow_from_directory( directory=r'C:\\CNN_multiclass_classifier\\Test',\n",
    "                                     target_size=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14802636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Huda': 0, 'Mahi': 1, 'Zoya': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_path.class_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511bba3c",
   "metadata": {},
   "source": [
    "# 3. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458e6b96",
   "metadata": {},
   "source": [
    "### 3.1 Build the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "972dd4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 128, 128, 8)       224       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 64, 64, 8)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 64, 64, 8)         584       \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 32, 32, 8)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8192)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               819300    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 3)                 303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 830,511\n",
      "Trainable params: 830,511\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(input_shape=(128, 128, 3), filters=8,kernel_size=(3,3), strides=1, padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2, padding='same'))\n",
    "model.add(Conv2D(filters=8,kernel_size=(3,3), strides=1, padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=2, padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=100,activation='relu'))\n",
    "model.add(Dense(units=100,activation='relu'))\n",
    "model.add(Dense(units=3,activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad54f957",
   "metadata": {},
   "source": [
    "# 3.2 Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46c84c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9382daa",
   "metadata": {},
   "source": [
    "# 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52162023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.1020 - accuracy: 0.3571 - val_loss: 1.3051 - val_accuracy: 0.3333\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.3077 - accuracy: 0.3929 - val_loss: 1.3797 - val_accuracy: 0.3333\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.2825 - accuracy: 0.3571 - val_loss: 1.1662 - val_accuracy: 0.3333\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0484 - accuracy: 0.3571 - val_loss: 1.1028 - val_accuracy: 0.1667\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9866 - accuracy: 0.6071 - val_loss: 1.1073 - val_accuracy: 0.3333\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.0202 - accuracy: 0.5000 - val_loss: 1.0960 - val_accuracy: 0.1667\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9136 - accuracy: 0.6786 - val_loss: 1.1074 - val_accuracy: 0.1667\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.9027 - accuracy: 0.5714 - val_loss: 1.0676 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8565 - accuracy: 0.6071 - val_loss: 1.0449 - val_accuracy: 0.5000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.8151 - accuracy: 0.6429 - val_loss: 1.0276 - val_accuracy: 0.3333\n"
     ]
    }
   ],
   "source": [
    "training_model = model.fit(x=training_data_path,batch_size=32,epochs=10,validation_data=validation_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7031b27",
   "metadata": {},
   "source": [
    "# 5. Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d37d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('image_recognitoion.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d2a2f",
   "metadata": {},
   "source": [
    "# 6. Model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f2db6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Huda': 0, 'Mahi': 1, 'Zoya': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data_path.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a14b3d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000020F47002430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "Huda\n"
     ]
    }
   ],
   "source": [
    "# Part 3 - Making new predictions\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing import image \n",
    "\n",
    "test_image = image.load_img(r\"C:\\CNN_multiclass_classifier\\Test\\Huda\\IMG_20220206_185332.jpg\",target_size = (128,128))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "\n",
    "# load model\n",
    "model = load_model('image_recognitoion.h5')\n",
    "result = model.predict(test_image)\n",
    "\n",
    "if result[0][0] == 1:\n",
    "    print('Huda')\n",
    "   \n",
    "elif result[0][1] ==1:\n",
    "    print('Mahi')\n",
    "    \n",
    "else:\n",
    "    print('Zoyu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441297f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
